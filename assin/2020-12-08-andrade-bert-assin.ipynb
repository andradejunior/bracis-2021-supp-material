{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbr 0.626~0.71~0.740\n",
    "ptpt 0.706~0.73~0.784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras similaridades: Cos, Ovl, Jacc, Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e métodos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:05.802172Z",
     "start_time": "2020-12-18T20:33:04.776846Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:05.824609Z",
     "start_time": "2020-12-18T20:33:05.805717Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xml.etree.ElementTree as et \n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    \"\"\"Parse xml to pandas dataframe.\"\"\"\n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot() \n",
    "\n",
    "    df_cols = ['id', 't', 'h', 'similarity']\n",
    "    rows = []\n",
    "\n",
    "    for node in xroot:\n",
    "        id_ = node.attrib.get(\"id\")\n",
    "        similarity = node.attrib.get(\"similarity\")\n",
    "        t = node.find(\"t\").text\n",
    "        h = node.find(\"h\").text\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": id_,\n",
    "            \"t\": t, \n",
    "            \"h\": h,\n",
    "            \"similarity\": similarity\n",
    "        })\n",
    "    return pd.DataFrame(rows, columns=df_cols, dtype=float)\n",
    "\n",
    "def eval_similarity(pairs_gold, pairs_sys):\n",
    "    '''\n",
    "    Evaluate the semantic similarity output of the system against a gold score. \n",
    "    Results are printed to stdout.\n",
    "    '''\n",
    "    \n",
    "    gold_values = np.array(pairs_gold)\n",
    "    sys_values = np.array(pairs_sys)\n",
    "    pearson = pearsonr(gold_values, sys_values)[0]\n",
    "    absolute_diff = gold_values - sys_values\n",
    "    mse = (absolute_diff ** 2).mean()\n",
    "    \n",
    "    print()\n",
    "    print('Similarity evaluation')\n",
    "    print('Pearson\\t\\tMean Squared Error')\n",
    "    print('-------\\t\\t------------------')\n",
    "    print('{:7.3f}\\t\\t{:18.2f}'.format(pearson, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:07.383956Z",
     "start_time": "2020-12-18T20:33:06.525784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assin-ptbr-dev.xml   assin-ptbr-train.xml  assin-ptpt-test.xml\r\n",
      "assin-ptbr-test.xml  assin-ptpt-dev.xml    assin-ptpt-train.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/assin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:07.974871Z",
     "start_time": "2020-12-18T20:33:07.386879Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ptbr_train = parse_xml('../data/assin/assin-ptbr-train.xml')\n",
    "df_ptbr_dev = parse_xml('../data/assin/assin-ptbr-dev.xml')\n",
    "df_ptbr_test = parse_xml('../data/assin/assin-ptbr-test.xml')\n",
    "\n",
    "df_ptpt_train = parse_xml('../data/assin/assin-ptpt-train.xml')\n",
    "df_ptpt_dev = parse_xml('../data/assin/assin-ptpt-dev.xml')\n",
    "df_ptpt_test = parse_xml('../data/assin/assin-ptpt-test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:07:03.872838Z",
     "start_time": "2020-12-18T00:07:03.868310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assin-ptbr-train: (2500, 4)\n",
      "assin-ptbr-dev: (500, 4)\n",
      "assin-ptbr-test: (2000, 4)\n",
      "\n",
      "assin-ptpt-train: (2500, 4)\n",
      "assin-ptpt-dev: (500, 4)\n",
      "assin-ptpt-test: (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'assin-ptbr-train: {df_ptbr_train.shape}')\n",
    "print(f'assin-ptbr-dev: {df_ptbr_dev.shape}')\n",
    "print(f'assin-ptbr-test: {df_ptbr_test.shape}')\n",
    "print()\n",
    "print(f'assin-ptpt-train: {df_ptpt_train.shape}')\n",
    "print(f'assin-ptpt-dev: {df_ptpt_dev.shape}')\n",
    "print(f'assin-ptpt-test: {df_ptpt_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:07:03.892099Z",
     "start_time": "2020-12-18T00:07:03.875493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>t</th>\n",
       "      <th>h</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>A gente faz o aporte financeiro, é como se a e...</td>\n",
       "      <td>Fernando Moraes afirma que não tem vínculo com...</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Em 2013, a história de como Walt Disney conven...</td>\n",
       "      <td>P.L.Travers era completamente contra a adaptaç...</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>David Silva bateu escanteio, Kompany escalou a...</td>\n",
       "      <td>David Silva cobrou escanteio, o zagueiro se ap...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Para os ambientalistas, as metas anunciadas pe...</td>\n",
       "      <td>Dilma aproveitou seu discurso ontem, na Confer...</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>De acordo com a PM, por volta das 10h30 havia ...</td>\n",
       "      <td>O protesto encerrou por volta de 12h15 (horári...</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                                  t  \\\n",
       "0  1.0  A gente faz o aporte financeiro, é como se a e...   \n",
       "1  2.0  Em 2013, a história de como Walt Disney conven...   \n",
       "2  3.0  David Silva bateu escanteio, Kompany escalou a...   \n",
       "3  4.0  Para os ambientalistas, as metas anunciadas pe...   \n",
       "4  5.0  De acordo com a PM, por volta das 10h30 havia ...   \n",
       "\n",
       "                                                   h  similarity  \n",
       "0  Fernando Moraes afirma que não tem vínculo com...        2.00  \n",
       "1  P.L.Travers era completamente contra a adaptaç...        2.25  \n",
       "2  David Silva cobrou escanteio, o zagueiro se ap...        3.75  \n",
       "3  Dilma aproveitou seu discurso ontem, na Confer...        2.75  \n",
       "4  O protesto encerrou por volta de 12h15 (horári...        2.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ptbr_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes pt-br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:37.994486Z",
     "start_time": "2020-12-18T20:33:37.952101Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            max_length: int = 60,\n",
    "            embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,\n",
    "    ):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        tokenized_text = self.tokenizer.encode_plus(text,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    max_length=self.max_length\n",
    "                                                    )[\"input_ids\"]\n",
    "\n",
    "        # Create an attention mask telling BERT to use all words\n",
    "        attention_mask = [1] * len(tokenized_text)\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return (\n",
    "            torch.tensor(tokenized_text).unsqueeze(0),\n",
    "            torch.tensor(attention_mask).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n",
    "        tokenized, attention_mask = self._tokenize(text)\n",
    "\n",
    "        embeddings = self.model(tokenized, attention_mask)\n",
    "        return self.embedding_func(embeddings)\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return torch.stack([self._tokenize_and_predict(string) for string in text])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguse-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:07:10.249284Z",
     "start_time": "2020-12-18T00:07:06.469640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:11:46.745769Z",
     "start_time": "2020-12-18T00:07:10.252019Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "t_embeddings = bert_transformer.transform(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = bert_transformer.transform(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:11:47.464203Z",
     "start_time": "2020-12-18T00:11:46.747854Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings.numpy(), h_embeddings.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:11:47.477819Z",
     "start_time": "2020-12-18T00:11:47.466179Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:11:47.492296Z",
     "start_time": "2020-12-18T00:11:47.483087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.444\t\t              1.97\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:11:47.508357Z",
     "start_time": "2020-12-18T00:11:47.496460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.475\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:12:10.463359Z",
     "start_time": "2020-12-18T00:11:47.510718Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:44:54.761180Z",
     "start_time": "2020-12-18T00:12:10.467485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "t_embeddings = bert_transformer.transform(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = bert_transformer.transform(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:44:55.719579Z",
     "start_time": "2020-12-18T00:44:54.764258Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings.numpy(), h_embeddings.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:44:55.728820Z",
     "start_time": "2020-12-18T00:44:55.723005Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:44:55.739328Z",
     "start_time": "2020-12-18T00:44:55.732143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.554\t\t              3.32\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:44:55.752961Z",
     "start_time": "2020-12-18T00:44:55.742440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.561\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:45:03.692248Z",
     "start_time": "2020-12-18T00:44:55.758062Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "word_embedding_model = models.Transformer(pretrained_model)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:23.253849Z",
     "start_time": "2020-12-18T00:45:03.695277Z"
    }
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:24.492190Z",
     "start_time": "2020-12-18T00:49:23.259041Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:24.501228Z",
     "start_time": "2020-12-18T00:49:24.495314Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:24.515343Z",
     "start_time": "2020-12-18T00:49:24.504860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.573\t\t              1.60\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:24.529611Z",
     "start_time": "2020-12-18T00:49:24.519209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.576\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T00:49:53.917060Z",
     "start_time": "2020-12-18T00:49:24.533295Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "word_embedding_model = models.Transformer(pretrained_model)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:55.143936Z",
     "start_time": "2020-12-18T00:49:53.920352Z"
    }
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:55.777593Z",
     "start_time": "2020-12-18T01:03:55.145785Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:55.782586Z",
     "start_time": "2020-12-18T01:03:55.779342Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:55.790603Z",
     "start_time": "2020-12-18T01:03:55.784413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.582\t\t              2.69\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:55.798381Z",
     "start_time": "2020-12-18T01:03:55.792455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.598\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sbert.net/docs/training/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:31:18.012771Z",
     "start_time": "2020-12-18T03:57:58.551887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853d58504a774911925be7b8d68680af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d66f89687b54d74b6e1cac13a749da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69cc3094c3142d0ac87bce74f361af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9ae10f5bcc4112a35896832e521423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f14a1d879e34ce8869e3ff7ad6405e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.757518170888842"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'bert-base-nli-mean-tokens' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "model_name = '/mnt/data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = '/mnt/data/sbert_base_finetuning_assin'# + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "#model = SentenceTransformer(model_name)\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for i, row in df_ptbr_train.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    train_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptbr_dev.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    dev_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptbr_test.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    test_samples.append(inp_example)\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:32:48.358429Z",
     "start_time": "2020-12-18T04:31:18.015494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:32:48.997982Z",
     "start_time": "2020-12-18T04:32:48.360209Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:32:49.002884Z",
     "start_time": "2020-12-18T04:32:48.999739Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:32:49.011507Z",
     "start_time": "2020-12-18T04:32:49.006297Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.763\t\t              0.35\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T04:32:49.020346Z",
     "start_time": "2020-12-18T04:32:49.014435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.758\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:22:27.896634Z",
     "start_time": "2020-12-18T04:32:49.022104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366ba5343310409b94cdfcccac19634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb74db71181421b9dc955f1976d2af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c9979ce3844db78310b5d59ad10645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc187229a2b4eb0abc2533a4759aea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1fddd9b01c4df2b8015a868145b74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.785509709865235"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'bert-base-nli-mean-tokens' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "model_name = '/mnt/data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = '/mnt/data/sbert_finetuning_assin'# + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "#model = SentenceTransformer(model_name)\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for i, row in df_ptbr_train.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    train_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptbr_dev.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    dev_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptbr_test.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    test_samples.append(inp_example)\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:27:28.762941Z",
     "start_time": "2020-12-18T06:22:27.900936Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptbr_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptbr_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:27:29.409787Z",
     "start_time": "2020-12-18T06:27:28.766245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:27:29.416315Z",
     "start_time": "2020-12-18T06:27:29.411353Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptbr_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:27:29.424324Z",
     "start_time": "2020-12-18T06:27:29.418444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.795\t\t              0.32\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T06:27:29.433225Z",
     "start_time": "2020-12-18T06:27:29.426965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.786\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes pt-pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:03:58.097232Z",
     "start_time": "2020-12-18T01:03:55.800462Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:08:35.712959Z",
     "start_time": "2020-12-18T01:03:58.099841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "t_embeddings = bert_transformer.transform(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = bert_transformer.transform(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:08:36.584498Z",
     "start_time": "2020-12-18T01:08:35.720472Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings.numpy(), h_embeddings.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:08:36.593770Z",
     "start_time": "2020-12-18T01:08:36.587226Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:08:36.602855Z",
     "start_time": "2020-12-18T01:08:36.596320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.379\t\t              2.46\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:08:36.615890Z",
     "start_time": "2020-12-18T01:08:36.606035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.408\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:33:56.786529Z",
     "start_time": "2020-12-18T20:33:46.880072Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "bert_transformer = BertTransformer(tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:50:27.451751Z",
     "start_time": "2020-12-18T20:33:56.790059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "t_embeddings = bert_transformer.transform(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = bert_transformer.transform(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:54:35.845518Z",
     "start_time": "2020-12-18T20:54:35.202007Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings.numpy(), h_embeddings.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:55:34.135824Z",
     "start_time": "2020-12-18T20:55:34.128194Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:55:35.316074Z",
     "start_time": "2020-12-18T20:55:35.310399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.521\t\t              4.16\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T20:55:36.187032Z",
     "start_time": "2020-12-18T20:55:36.180391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.530\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:20:01.936259Z",
     "start_time": "2020-12-18T01:19:59.771423Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "word_embedding_model = models.Transformer(pretrained_model)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:32.457389Z",
     "start_time": "2020-12-18T01:20:01.937903Z"
    }
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:33.095997Z",
     "start_time": "2020-12-18T01:21:32.459068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:33.101419Z",
     "start_time": "2020-12-18T01:21:33.097832Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:33.109790Z",
     "start_time": "2020-12-18T01:21:33.103422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.531\t\t              2.22\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:33.119114Z",
     "start_time": "2020-12-18T01:21:33.111911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.537\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:21:39.626451Z",
     "start_time": "2020-12-18T01:21:33.121489Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "\n",
    "pretrained_model = '../data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "word_embedding_model = models.Transformer(pretrained_model)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:28:32.451159Z",
     "start_time": "2020-12-18T01:21:39.628279Z"
    }
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:28:33.093411Z",
     "start_time": "2020-12-18T01:28:32.452859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:28:33.098931Z",
     "start_time": "2020-12-18T01:28:33.095270Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:28:33.107384Z",
     "start_time": "2020-12-18T01:28:33.101001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.491\t\t              3.57\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:28:33.116963Z",
     "start_time": "2020-12-18T01:28:33.109687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.512\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Sentence-BERT\n",
    "\n",
    "https://www.sbert.net/docs/training/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-portuguese-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:01:59.947707Z",
     "start_time": "2020-12-18T06:27:29.435281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0ee7bd10a0405b93db75a7c5a05934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f8b8e8657a46438296658a862c3ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03213285dcfc4f06b28cf3488ad3b7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca40e63114794dd29383dbfd1872c05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190c225be82741e8b3469b0f13efa506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7594120600586469"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'bert-base-nli-mean-tokens' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "model_name = '/mnt/data/PyTorch checkpoint/bert-base-portuguese-cased_pytorch_checkpoint'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = '/mnt/data/sbert_base_ptpt_finetuning_assin'# + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "#model = SentenceTransformer(model_name)\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for i, row in df_ptpt_train.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    train_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptpt_dev.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    dev_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptpt_test.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    test_samples.append(inp_example)\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:03:33.780893Z",
     "start_time": "2020-12-18T07:01:59.949364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:03:34.439667Z",
     "start_time": "2020-12-18T07:03:33.782667Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:03:34.444343Z",
     "start_time": "2020-12-18T07:03:34.441357Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:03:34.451478Z",
     "start_time": "2020-12-18T07:03:34.446441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.765\t\t              0.61\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T07:03:34.461067Z",
     "start_time": "2020-12-18T07:03:34.453564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.759\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-large-portuguse-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:00:47.081313Z",
     "start_time": "2020-12-18T07:03:34.463341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d576cd2b162445d9a905314cc97e695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff5726fd10941f6a8b167fa647f7a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01f09f1b7764770b047f064ea0c60a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d718d058c14485290f19d0fbabe3fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88baefc2b54e3a96111cf3e88abae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=157.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7817660396216041"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'bert-base-nli-mean-tokens' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "model_name = '/mnt/data/PyTorch checkpoint/bert-large-portuguese-cased_pytorch_checkpoint'\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = '/mnt/data/sbert_ptpt_finetuning_assin'# + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "#model = SentenceTransformer(model_name)\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for i, row in df_ptpt_train.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    train_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptpt_dev.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    dev_samples.append(inp_example)\n",
    "    \n",
    "for i, row in df_ptpt_test.iterrows():\n",
    "    inp_example = InputExample(texts=[row['t'], row['h']], label=row['similarity'] / 5)\n",
    "    test_samples.append(inp_example)\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:06:05.751641Z",
     "start_time": "2020-12-18T09:00:47.085094Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_embeddings = model.encode(df_ptpt_test['t'].tolist())\n",
    "h_embeddings = model.encode(df_ptpt_test['h'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:06:06.399700Z",
     "start_time": "2020-12-18T09:06:05.753507Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "similarities = [5.0 * cosine_similarity([t], [h])[0][0] for t, h in zip(t_embeddings, h_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:06:06.404568Z",
     "start_time": "2020-12-18T09:06:06.401432Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs_gold = df_ptpt_test['similarity'].tolist()\n",
    "pairs_sys = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:06:06.412515Z",
     "start_time": "2020-12-18T09:06:06.406424Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity evaluation\n",
      "Pearson\t\tMean Squared Error\n",
      "-------\t\t------------------\n",
      "  0.782\t\t              0.68\n"
     ]
    }
   ],
   "source": [
    "eval_similarity(pairs_gold, pairs_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:06:06.420698Z",
     "start_time": "2020-12-18T09:06:06.414312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation:   0.782\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print('Spearman correlation: {:7.3f}'.format(spearmanr(pairs_gold, pairs_sys)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
